<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project Proposal</title>
    <!-- Add any additional styles or links to external stylesheets here -->
  </head>
  <body>
    <!-- Project Information Section -->
    <section>
      <h1>Project Proposal</h1>
      <p><strong>Name(s):</strong> Aidan Brem, Ruby Tseng, Joost Vonk</p>
      <p>
        <strong>Project Title:</strong> VisAR: Augmented Reality Navigation for
        the Visually Impaired
      </p>
    </section>

    <!-- Project Description Section -->
    <section>
      <h2>Project Description</h2>
      <p>
        Our project, VisAR, aims to enhance the navigation experience for
        visually impaired individuals by using Augmented Reality (AR)
        technology. Our AR system comprises four integral components designed to
        address the unique challenges faced by the visually impaired community.
        Firstly, the real-time environmental awareness module utilizes sensors
        on Meta Quest and spatial mapping to provide users with detailed
        information about their surroundings. Secondly, the proximity detection
        feature ensures users' safety by alerting them to obstacles and
        potential hazards in their path, fostering a secure and confident
        mobility experience. Additionally, our system incorporates auditory and
        haptic cues to offer intuitive navigation guidance, enhancing spatial
        awareness and promoting an efficient and fluid movement through various
        environments. Lastly, the integration of specialized audio technology
        tailors the auditory feedback to the specific needs of visually impaired
        users while navigating the space. Through these methods, our project
        seeks to not only enhance the day-to-day mobility of visually impaired
        individuals but also empower them with increased independence and
        confidence in navigating their surroundings.
      </p>
    </section>

    <!-- Goals Section -->
    <section>
      <h2>Goals</h2>
      <ul>
        <li>
          Scan/generate model of living area: Areas must be scanned by
          developers to provide users with an accurate model of their
          surroundings with AR information overlaid on top of it
        </li>
        <li>
          Proximity Detection: User will be alerted to proximity to obstacles
          via the AR system scan through auditory and haptic feedback to assist
          with avoidance
        </li>
        <li>
          Spatialized Audio: Some auditory feedback will be spatialized,
          allowing visually impaired users to use auditory feedback for distance
          assessment
        </li>
        <li>
          Feedback adjustment interface: User will be able to customize the type
          and intensity of the feedback transmitted to ensure a comfortable
          experience
        </li>
        <li>
          Object of interest identification (Stretch goal): Object of Interest
          Identification: User will be able to be alerted to objects of interest
          via the AR system scan through auditory and haptic feedback to assist
          with identification
        </li>
      </ul>
    </section>

    <!-- Timeline Section -->
    <section>
      <h2>Timeline</h2>
      <ul>
        <li>Feb 2: Research</li>
        <li>Feb 9: Spatial Modeling</li>
        <li>Feb 23: Proximity Detection</li>
        <li>March 1: Auditory/Haptic Cues</li>
        <li>March 8: Spatialized Audio</li>
        <li>March 15: Object of Interest ID/Contingency Time</li>
      </ul>
    </section>
  </body>
</html>
